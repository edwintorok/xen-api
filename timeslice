caml_thread_tick

What is it?

OCaml 4's global runtime lock and OCaml'5 domain runtime lock only allows one OCaml thread at a time to run.
The tick thread periodically switches between these threads to prevent one thread from blocking progress for too long.
The tick thread is created when more than 1 OCaml thread exists (in a domain).

What is wrong with it?

Even when the application is completely idle (e.g. waiting for an event) it will wake up the system every 50ms.

According to https://www.intel.com/content/www/us/en/developer/articles/tool/powertop-primer.html
the CPU needs to spend 50ms+ in C4, and spending 20ms instead of 4.4ms in C4 gives an extra hour of battery life.

If you run any multithreaded OCaml 4 application then the 1st constraint is not met already, and if you run 3 or more applications then the 2nd one isn't either.
With OCaml 5 the frequency of wakeups is increased if you run multiple threads per domain,
e.g. 14 core laptop -> 280 wakeups/s

Also the sleep might be interrupted by a signal, which would then result in 2 yields: one for handling the signal, and then another from the tick thread almost immediately, although that is not necessarily bad if we were waiting for that signal in another thread (e.g. SIGIO), but if we receive a lot of signals then this may result in a lot of spurious yields.

How to fix it?

It currently uses 'select' to sleep, but we could use clock_nanosleep on CLOCK_PROCESS_CPUTIME_ID,
which measures CPU used by the process. If the process is idle then we won't get woken up.
And we still realise the original goal of switching threads to prevent blocking progress, but only if we are careful to release the runtime lock around blocking system calls.
If we call any system call that blocks for a long time, and don't release the runtime lock then we could potentially starve other processes, because we won't get interrupted with EINTR either.
Although that would've been a problem previously too, because we don't actually send a signal to the process (to cause the syscall to be interrupted), we just record that we have a pending signal (OCaml 4), or pending interrupt (OCaml 5) by changing the minor heap limit.

We could make the select / nanosleep switch based on OCAMLRUNPARAM env var, so that we can always switch back to the old behaviour, e.g. for testing purposes.

In fact we could take this further and remove the tick thread completely and use per thread interval timers,
that interrupt OCaml threads with a signal when they used too much CPU, that way we only interrupt the thread that is using CPU and not others.
However we can keep the process-level timer as a backup (e.g. in case we failed to create a timer for a thread). The number of timers available might be limited, although the limit is usually high (e.g. 253153 on Linux)

The interval timer can be implemented at the application level instead. There are 3 timers of interest:
* ITIMER_VIRTUAL - only looks at user time consumed by the process, which would be relevant for us, because we want to limit the time spent by OCaml threads. If they release the runtime lock correctly around costly system calls then we don't need to account for system threads, the runtime lock would've already been released there. (although in general we can't know this, and there are pending patches to XAPI to release the lock where needed)
* ITIMER_PROF - looks at user + system time

* Posix CLOCK_PROCESS_CPUTIME_ID
* Posix CLOCK_THREAD_CPUTIME_ID

There is only 1 itimer  / process, so this approach wouldn't work for the OCaml runtime, the others timers would work though.

The resolution of these timers is different, e.g. the itimers may only be accurate up to scheduler ticks, i.e. on Linux 1/HZ (10ms, 4ms, 1ms), they will be rounded up though.

For an application using itimer is the easiest, since it requires no extra C bindings, the problem is that ITIMER_VIRTUAL sends SIGVTALRM, which is also the (fake) signal used in OCaml 4, so we *must* always yield when we receive it.
With the other timers we could dynamically chose to yield or not, e.g. based on whether anything is waiting for IO or not, or whether we got a SIGIO or not.

TODO: workload that measures fairness, hiccups, etc.

TODO: we need to run kernel selftests, especially the timer ones, etc. kernel-selftests-internal doesn't enable timers in Fedora, check what we do?

Problem:

when we have CPU intensive OCaml threads then Unix call latency is very high, as we may need to run several other threads before we get a chance to respond.
TODO: the test program that measured 1.5s+ tail latency 99%. 

Mitigations:
 * reduce total number of CPU intensive threads that are active at a time
 * reduce timer interval (though of course this increases switching overhead, which we'll need to measure)
 * use a signal (either SIGIO from kernel, or caml_record_signal in a C stub when system call has returned)
 * if we are sure we have nonblocking mode set on an fd then we can make a syscall without releasing the runtime lock, and only release runtime lock if we sleep. Could use MSG_DONTWAIT, but that is Linux specific.
 * low latency: could use the URG portion of the socket... but would require modifying the other side too, as otherwise it wouldn't know how to receive it. fcntl with F_SETOWN can be used to enable receipt of SIGURG signals.
   Urgent data can be send with MSG_OOB, and received same way, unless SO_OOBINLINE is set on the socket (which we could set...). We could probably patch XenAPI.py to send/receive OOB data. This is different from SIGIO which would tell you when IO is possible (i.e. there is buffer space), but is not a completion signal.
   SIGURG will be targeted to the IO thread, but OCaml will record the presence of signal, and if another thread is busy then that one will handle it. So we can yield there, but we need to be careful to not yield on the one that handles the IO...

   Although note that TCP had some issues in how urgent data was defined that was fixed in later RFCs, but it hasn't actually been implemented, and although a sysctl is provided, it is only for one direction
   , also middle boxes may clear it. In fact the RFC says new applications shouldn't use it, but existing implementations must still support it, or if we use it then also set the oob_inline flag.
   https://datatracker.ietf.org/doc/html/rfc6093

   SIGURG (or a caml_record_signal on return from the syscall) interrupts other running threads, but still doesn't guarantee that the thread handling the IO runs next.

   Also how about python? xmlrpc client httpconnection transport, how to make it set oob?

* To bound the execution time we can try to use a semaphore around Thread.yield, which effectively limits how many yields we go through before getting back to the IO (not CPU intensive thread)...

TODO: also test cache effect with bechamel perf?

Autotune executable...

semaphore around thread.yield: could lead to deadlocks if we're holding another lock that someone else is waiting for us to release, TODO: testcase for this... although it shouldn't because the yield should eventually return and we'd release the locks...


Timeslices

# C states

C states have a target residency and exit latency, see https://www.kernel.org/doc/html/v5.0/admin-guide/pm/cpuidle.html#the-idle-loop

The timeslice interrupt in the OCaml tick thread should be long enough to allow the CPU package to enter the deepest C state without having to exit to handle the timer tick.
This means that we should require:
 timer_tick_interval_ms >  1000 * max(target_residency + exit_latency)

(the residency values in the Linux kernel are in microseconds)

The residency values can be found in intel_idle.c, and looking at kernel 6.9:
Atom Silvermont/Airmonr (byt, cht, tangier): 30ms
Atom Goldmont (bxt): 20ms
Haswell: (hsw), Broadwell (bdw): 7.6ms
Skylake (skl) : 5.89ms

---

Icelake (icx): 0.77ms
Sapphire rapids (spr): 1.09ms
Alderlake: 2.68ms
...

However some of these CPUs are no longer supported, and we should focus our optimizations on supported CPUs.
A page that collects this for Intel:
https://www.intel.com/content/www/us/en/support/articles/000022396/processors.html

Also for some CPUs the information comes from ACPI

This information is probably best queried at install / start time.

# Cache hotness


https://metebalci.com/blog/a-minimum-complete-tutorial-of-cpu-power-management-c-states-and-p-states/
Worst case L3 cache would be lost, and had to be refilled.

However for the purpose of the tick thread we should only look at L2 cache refill times (from L3).

https://info.citrite.net/display/engp/Summary+of+Best+Practice+of+Toolstack#SummaryofBestPracticeofToolstack-Moreonnaming

Taking one CPU as an example, if we have a 256KiB L2 cache, and we can do 29 bytes / cycle, then it will take 9039 cycles to refill,
and if we assume a worst case base frequency of 2Ghz, that will be <5us.
However if the other thread ran on another CPU socket then we might need to refill L3.

Again taking same example, that would be 2MiB, and 18byte/cycle, so that'd be ~60us. For this to have <1% impact: 6ms timeslice minimum.

Although that wouldn't explain why other papers have come to the conclusion that a 2.5ms or 5ms timeslice may have an ~50% perf impact on HMMER (compared to 10ms) 
The above L* cache timings are for a sustained copy, but individual accesses will incurr a higher latency overhead.

Calculating the worst case latency:
L2: 12 cycle for 64 bytes, 256KiB -> 24us -> we need a 2.4ms timeslice for this not to have an impact
L3: 44 cycle for 64 bytes, 2MiB -> 720us -> for this to have <1% impact we need a 72ms timeslice

There is other scheduling overhead beyond losing caches, which is probably a larger contributing factor.
We should measure that ovearhead for our kernel + OCaml

https://memlab.ece.gatech.edu/papers/VMTJ_2015_1.pdf

look at references [15] and [16], it shows that deflate compression has 7ms total context switch overhead, on a 120Mhz ARM CPU.
See also "Quantifying The Cost of Context Switch", but that was done on old CPUs too

There are other effects, such as branch predictors needing to relearn when switching VMs


We could have a RRUL/Flent like test which implements the IRTT protocol to measure latency over calling a unix socket,
while under load, and that load might be the netperf threads? or just use hdr histogram for now.

TODO: extend unixlatency to measure latency to send OPTIONS to /, and get a full reply

/dev/cpu_dma_latency is good on native Linux, but won't work for Xen (we need to do it via xenpm)

Be careful with sched-deadline, it can introduce massive latencies (worse than sched_other) if runtime is exceeded (essentially it'll force waiting until period, *even if the cpu is idle*).
sched_rr is better.
