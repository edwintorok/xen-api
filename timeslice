caml_thread_tick

What is it?

OCaml 4's global runtime lock and OCaml'5 domain runtime lock only allows one OCaml thread at a time to run.
The tick thread periodically switches between these threads to prevent one thread from blocking progress for too long.
The tick thread is created when more than 1 OCaml thread exists (in a domain).

What is wrong with it?

Even when the application is completely idle (e.g. waiting for an event) it will wake up the system every 50ms.

According to https://www.intel.com/content/www/us/en/developer/articles/tool/powertop-primer.html
the CPU needs to spend 50ms+ in C4, and spending 20ms instead of 4.4ms in C4 gives an extra hour of battery life.

If you run any multithreaded OCaml 4 application then the 1st constraint is not met already, and if you run 3 or more applications then the 2nd one isn't either.
With OCaml 5 the frequency of wakeups is increased if you run multiple threads per domain,
e.g. 14 core laptop -> 280 wakeups/s

Also the sleep might be interrupted by a signal, which would then result in 2 yields: one for handling the signal, and then another from the tick thread almost immediately, although that is not necessarily bad if we were waiting for that signal in another thread (e.g. SIGIO), but if we receive a lot of signals then this may result in a lot of spurious yields.

How to fix it?

It currently uses 'select' to sleep, but we could use clock_nanosleep on CLOCK_PROCESS_CPUTIME_ID,
which measures CPU used by the process. If the process is idle then we won't get woken up.
And we still realise the original goal of switching threads to prevent blocking progress, but only if we are careful to release the runtime lock around blocking system calls.
If we call any system call that blocks for a long time, and don't release the runtime lock then we could potentially starve other processes, because we won't get interrupted with EINTR either.
Although that would've been a problem previously too, because we don't actually send a signal to the process (to cause the syscall to be interrupted), we just record that we have a pending signal (OCaml 4), or pending interrupt (OCaml 5) by changing the minor heap limit.

We could make the select / nanosleep switch based on OCAMLRUNPARAM env var, so that we can always switch back to the old behaviour, e.g. for testing purposes.

In fact we could take this further and remove the tick thread completely and use per thread interval timers,
that interrupt OCaml threads with a signal when they used too much CPU, that way we only interrupt the thread that is using CPU and not others.
However we can keep the process-level timer as a backup (e.g. in case we failed to create a timer for a thread). The number of timers available might be limited, although the limit is usually high (e.g. 253153 on Linux)


Timeslices

# C states

C states have a target residency and exit latency, see https://www.kernel.org/doc/html/v5.0/admin-guide/pm/cpuidle.html#the-idle-loop

The timeslice interrupt in the OCaml tick thread should be long enough to allow the CPU package to enter the deepest C state without having to exit to handle the timer tick.
This means that we should require:
 timer_tick_interval_ms >  1000 * max(target_residency + exit_latency)

(the residency values in the Linux kernel are in microseconds)

The residency values can be found in intel_idle.c, and looking at kernel 6.9:
Atom Silvermont/Airmonr (byt, cht, tangier): 30ms
Atom Goldmont (bxt): 20ms
Haswell: (hsw), Broadwell (bdw): 7.6ms
Skylake (skl) : 5.89ms

---

Icelake (icx): 0.77ms
Sapphire rapids (spr): 1.09ms
Alderlake: 2.68ms
...

However some of these CPUs are no longer supported, and we should focus our optimizations on supported CPUs.
A page that collects this for Intel:
https://www.intel.com/content/www/us/en/support/articles/000022396/processors.html

Also for some CPUs the information comes from ACPI

This information is probably best queried at install / start time.

# Cache hotness


https://metebalci.com/blog/a-minimum-complete-tutorial-of-cpu-power-management-c-states-and-p-states/
Worst case L3 cache would be lost, and had to be refilled.

However for the purpose of the tick thread we should only look at L2 cache refill times (from L3).

https://info.citrite.net/display/engp/Summary+of+Best+Practice+of+Toolstack#SummaryofBestPracticeofToolstack-Moreonnaming

Taking one CPU as an example, if we have a 256KiB L2 cache, and we can do 29 bytes / cycle, then it will take 9039 cycles to refill,
and if we assume a worst case base frequency of 2Ghz, that will be <5us.
However if the other thread ran on another CPU socket then we might need to refill L3.

Again taking same example, that would be 2MiB, and 18byte/cycle, so that'd be ~60us. For this to have <1% impact: 6ms timeslice minimum.

Although that wouldn't explain why other papers have come to the conclusion that a 2.5ms or 5ms timeslice may have an ~50% perf impact on HMMER (compared to 10ms) 
The above L* cache timings are for a sustained copy, but individual accesses will incurr a higher latency overhead.

Calculating the worst case latency:
L2: 12 cycle for 64 bytes, 256KiB -> 24us -> we need a 2.4ms timeslice for this not to have an impact
L3: 44 cycle for 64 bytes, 2MiB -> 720us -> for this to have <1% impact we need a 72ms timeslice

There is other scheduling overhead beyond losing caches, which is probably a larger contributing factor.
We should measure that ovearhead for our kernel + OCaml

https://memlab.ece.gatech.edu/papers/VMTJ_2015_1.pdf

look at references [15] and [16], it shows that deflate compression has 7ms total context switch overhead, on a 120Mhz ARM CPU.
See also "Quantifying The Cost of Context Switch", but that was done on old CPUs too

There are other effects, such as branch predictors needing to relearn when switching VMs


We could have a RRUL/Flent like test which implements the IRTT protocol to measure latency over calling a unix socket,
while under load, and that load might be the netperf threads? or just use hdr histogram for now.

TODO: extend unixlatency to measure latency to send OPTIONS to /, and get a full reply

/dev/cpu_dma_latency is good on native Linux, but won't work for Xen (we need to do it via xenpm)

Be careful with sched-deadline, it can introduce massive latencies (worse than sched_other) if runtime is exceeded (essentially it'll force waiting until period, *even if the cpu is idle*).
sched_rr is better.
